
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_example/plot_example_MOE.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_example_plot_example_MOE.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_example_plot_example_MOE.py:


Use of Mixture of Experts
-------------------------

.. GENERATED FROM PYTHON SOURCE LINES 7-15

.. code-block:: default


    from smt.sampling_methods import LHS
    from smt.problems import Sphere
    from smt.applications import MOE
    import numpy as np
    import otsmt









.. GENERATED FROM PYTHON SOURCE LINES 16-17

| Definition of Initial data

.. GENERATED FROM PYTHON SOURCE LINES 17-31

.. code-block:: default


       
    # Construction of the DOE
    fun = Sphere(ndim=2)
    sampling = LHS(xlimits=fun.xlimits, criterion="m")
    xt = sampling(40)
    yt = fun(xt)
    # Compute the gradient
    for i in range(2):
        yd = fun(xt, kx=i)
        yt = np.concatenate((yt, yd), axis=1)
       
    xv= sampling(10)
    







.. GENERATED FROM PYTHON SOURCE LINES 32-33

| Training of smt model for  Mixture of Experts

.. GENERATED FROM PYTHON SOURCE LINES 33-38

.. code-block:: default


    moe = MOE(n_clusters=2)
    moe.set_training_values(xt, yt[:,0][:,np.newaxis])
    moe.train()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Kriging 0.0005594748972441721
    LS 17.567295398251957
    QP 4.689582056016661e-13
    KPLS 0.0002794618784349723
    KPLSK 0.0005594748972441721
    RBF 74.4177792519404
    RMTC 5.416563213316962
    RMTB 4.883705352175994
    IDW 8.810799896936828
    Best expert = QP
    Kriging 0.0023037378955540126
    LS 36.075984494007564
    QP 1.0921344645665981e-13
    KPLS 0.002915166931659202
    KPLSK 0.0023037378955540126
    RBF 55.31104848981852
    RMTC 0.9965883935571512
    RMTB 1.1558651747370827
    IDW 5.310437180295213
    Best expert = QP




.. GENERATED FROM PYTHON SOURCE LINES 39-40

| Creation of OpenTurns PythonFunction for prediction

.. GENERATED FROM PYTHON SOURCE LINES 40-44

.. code-block:: default


    otmoe = otsmt.smt2ot(moe)
    otmoeprediction = otmoe.getPredictionFunction()
    print('Predicted values by MOE:',otmoeprediction(xv))   




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Predicted values by MOE:     [ y0       ]
    0 : [  43.5068 ]
    1 : [  57.7711 ]
    2 : [  62.1809 ]
    3 : [ 141.923  ]
    4 : [  68.1371 ]
    5 : [  82.5268 ]
    6 : [  31.1796 ]
    7 : [  11.1622 ]
    8 : [  27.6123 ]
    9 : [ 146.365  ]





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  1.065 seconds)


.. _sphx_glr_download_auto_example_plot_example_MOE.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_example_MOE.py <plot_example_MOE.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_example_MOE.ipynb <plot_example_MOE.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
